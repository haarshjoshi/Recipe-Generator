# Recipe-Generator
In this project, I have generated 4 deep neural model for recipe generation from the dataset which contained ingredients and recipes. The main aim is to make Seq-2-Seq model which takes ingredients as input in the encoder and output recipe from the decoder. So basically, I have created an 4 different Encoder-Decoder architecture for the same data in order to compare the difference and improvements in each of the architecture.
### Model and Training Configurations
* Baseline 1: This is a Seq-2-Seq model without attention, i.e., the ingredient sample goes into the encoder LSTM. The output from final hidden layer of the encoder is then fed to the initial state of the decoder LSTM.
* Baseline 2: This is a Seq-2-Seq model with attention. Unlike the Baseline 1, this model instead of using final hidden layers output into the initial state of encoder, uses attention scores which are calculated for each hidden state of the encoder so that emphasis is placed on each hidden state rather than just the final hidden state of the encoder. This approach is helpful when dealing with long sentences fed into the encoder.
* Extension 1: Based on performance, Baseline 1 performed better than Baseline 2 and hence was used to extend the model further. In this model, some pre-processing to the ingredient-recipe pairs is applied. After the pre-processing, while implementing the encoder-decoder, extra LSTM layers are included in both the encoder as well as decoder. Extra LSTM layers helps in leaning complex structure in the input and give a better output.
* Extension 2: In this model, the same pre-processing steps are applied as Extension 1. The main problem with seq-2-seq models is that they face the problem of repetition, and this is common when generating multi sentence text. Coverage mechanism can be used to handle this situation. In this model, we use a coverage vector which takes sum of attention distribution over the hidden states at each timestep. This vector helps in getting word coverage that the model has received at each timestep. Initially, it is a zero vector as no input has been received at the decoder at timestep 0. The coverage vector is used as an extra input to the decoder in addition to the input from the encoder. This helps the decoder to keep track of what has already been output by decoder so that it does not output the same words again leading to non-repetitive sentence. Also, coverage loss is also calculated to optimize the coverage vector. The total loss of the model is finally a mixture of model loss and the coverage loss.
### Quantitative Evaluation
The following metrics are used to perform evaluation of the 4 models:
* BLEU: compares generated sentence with reference sentence (provided by humans). It considers sequences with n-grams
* METEOR: like BLEU but is more complex as it considers synonyms, word order as well. In short it also measures the similarity between generated text and reference text but instead of n-gram sequence considers word order, synonyms etc. This helps in understanding meaning and structure of the sentence.
* Avg% given items:  the percentage of how many numbers of ingredients which are provided to the encoder-decoder are being used in the recipe after recipe generation.
* Avg extra items: how many extra items are being used on average in the recipe which are not even there in ingredients.
